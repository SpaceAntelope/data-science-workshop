{
 "cells": [
    {
    "cell_type": "code",
    "execution_count": 1,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Import basic libraries and keras\n",
"import os\n",
"import json\n",
"import numpy as np\n",
"from keras.preprocessing.text import Tokenizer\n",
"from keras.models import Sequential, load_model\n",
"from keras.preprocessing.sequence import pad_sequences\n",
"from keras.layers import Dense, Embedding, Conv1D, GlobalMaxPooling1D\n",
"\n",
"# Load input data\n",
"training = np.genfromtxt('15000tweets.csv', delimiter=',', skip_header=1, usecols=(1, 3), dtype=None, encoding='utf-8')\n",
"\n",
"# Get tweets and sentiments (0 or 1)\n",
"train_x = [str(x[1]) for x in training]\n",
"train_y = np.asarray([x[0] for x in training])\n",
"\n",
"# Use the 3000 most popular words found in our dataset\n",
"max_words = 3000\n",
"\n",
"# Tokenize the data\n",
"tokenizer = Tokenizer(num_words=max_words)\n",
"tokenizer.fit_on_texts(train_x)\n",
"dictionary = tokenizer.word_index\n",
"# Save tokenizer dictionary to file\n",
"if not os.path.exists('dictionary.json'):\n",
"    with open('dictionary.json', 'w') as outfile:\n",
"        json.dump(tokenizer.word_index, outfile)\n",
"\n",
"# For each tweet, change each token to its ID in the Tokenizer's word_index\n",
"sequences = tokenizer.texts_to_sequences(train_x)\n",
"train_x = pad_sequences(sequences, maxlen=300)\n",
"\n",
"# Check if there is a pre-trained model\n",
"if not os.path.exists('cnn_model.h5'):\n",
"    # Create a neural network with 3 dense layers\n",
"    model = Sequential()\n",
"    model.add(Embedding(3000, 64, input_length=300))\n",
"    model.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
"    model.add(GlobalMaxPooling1D())\n",
"    model.add(Dense(256, activation='relu'))\n",
"    model.add(Dense(1, activation='sigmoid'))\n",
"    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
"\n",
"    # Train the model\n",
"    model.fit(train_x, train_y, batch_size=32, epochs=5, verbose=1, validation_split=0.1, shuffle=True)\n",
"\n",
"    # Save the model\n",
"    model.save('cnn_model.h5')\n",
"else:\n",
"    # Load the model from disk\n",
"    model = load_model('cnn_model.h5')\n",
"\n",
"\n"
    ]
    }],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
