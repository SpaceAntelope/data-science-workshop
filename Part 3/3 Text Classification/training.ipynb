{
 "cells": [
    {
    "cell_type": "code",
    "execution_count": 1,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Import basic libraries and keras\n",
"import os\n",
"import json\n",
"import keras\n",
"import numpy as np\n",
"import keras.preprocessing.text as kpt\n",
"from keras.layers import Dense, Dropout\n",
"from keras.preprocessing.text import Tokenizer\n",
"from keras.models import Sequential, load_model\n",
"\n",
"# Load input data\n",
"training = np.genfromtxt('15000tweets.csv', delimiter=',', skip_header=1, usecols=(1, 3), dtype=None, encoding='utf-8')\n",
"\n",
"# Get tweets and sentiments (0 or 1)\n",
"train_x = [str(x[1]) for x in training]\n",
"train_y = np.asarray([x[0] for x in training])\n",
"\n",
"# Use the 3000 most popular words found in our dataset\n",
"max_words = 3000\n",
"\n",
"# Tokenize the data\n",
"tokenizer = Tokenizer(num_words=max_words)\n",
"tokenizer.fit_on_texts(train_x)\n",
"dictionary = tokenizer.word_index\n",
"# Save tokenizer dictionary to file\n",
"if not os.path.exists('dictionary.json'):\n",
"    with open('dictionary.json', 'w') as outfile:\n",
"        json.dump(tokenizer.word_index, outfile)\n",
"\n",
"# For each tweet, change each token to its ID in the Tokenizer's word_index\n",
"allWordIndices = []\n",
"for text in train_x:\n",
"    words = kpt.text_to_word_sequence(text)\n",
"    wordIndices = [dictionary[word] for word in words]\n",
"    allWordIndices.append(wordIndices)\n",
"\n",
"# Create matrix with indexed tweets and categorical target\n",
"train_x = tokenizer.sequences_to_matrix(np.asarray(allWordIndices), mode='binary')\n",
"train_y = keras.utils.to_categorical(train_y, 2)\n",
"\n",
"# Check if there is a pre-trained model\n",
"if not os.path.exists('model.h5'):\n",
"    # Create a neural network with 3 dense layers\n",
"    model = Sequential()\n",
"    model.add(Dense(512, input_shape=(max_words,), activation='relu'))\n",
"    model.add(Dropout(0.5))\n",
"    model.add(Dense(256, activation='sigmoid'))\n",
"    model.add(Dropout(0.5))\n",
"    model.add(Dense(2, activation='softmax'))\n",
"    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
"\n",
"    # Train the model\n",
"    model.fit(train_x, train_y, batch_size=32, epochs=5, verbose=1, validation_split=0.1, shuffle=True)\n",
"\n",
"    # Save the model\n",
"    model.save('model.h5')\n",
"else:\n",
"    # Load the model from disk\n",
"    model = load_model('model.h5')\n",
"\n",
"\n"
    ]
    }],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
