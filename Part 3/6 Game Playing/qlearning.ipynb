{
 "cells": [
    {
    "cell_type": "code",
    "execution_count": 1,
    "metadata": {},
    "outputs": [],
    "source": [
        "import numpy as np\n",
"from qcatch import display_screen\n",
"\n",
"class ExperienceReplay(object):\n",
"    \"\"\"\n",
"    During gameplay all the experiences < s, a, r, s’ > are stored in a replay memory.\n",
"    In training, batches of randomly drawn experiences are used to generate the input and target for training.\n",
"    \"\"\"\n",
"    def __init__(self, max_memory=100, discount=.9):\n",
"        \"\"\"\n",
"        Setup\n",
"        max_memory: the maximum number of experiences we want to store\n",
"        memory: a list of experiences\n",
"        discount: the discount factor for future experience\n",
"\n",
"        In the memory the information whether the game ended at the state is stored seperately in a nested array\n",
"        [...\n",
"        [experience, game_over]\n",
"        [experience, game_over]\n",
"        ...]\n",
"        \"\"\"\n",
"        self.max_memory = max_memory\n",
"        self.memory = list()\n",
"        self.discount = discount\n",
"\n",
"    def remember(self, states, game_over):\n",
"        #Save a state to memory\n",
"        self.memory.append([states, game_over])\n",
"        #We don't want to store infinite memories, so if we have too many, we just delete the oldest one\n",
"        if len(self.memory) > self.max_memory:\n",
"            del self.memory[0]\n",
"\n",
"    def get_batch(self, model, batch_size=10):\n",
"\n",
"        #How many experiences do we have?\n",
"        len_memory = len(self.memory)\n",
"\n",
"        #Calculate the number of actions that can possibly be taken in the game\n",
"        num_actions = model.output_shape[-1]\n",
"\n",
"        #Dimensions of the game field\n",
"        env_dim = self.memory[0][0][0].shape[1]\n",
"\n",
"        #We want to return an input and target vector with inputs from an observed state...\n",
"        inputs = np.zeros((min(len_memory, batch_size), env_dim))\n",
"\n",
"        #...and the target r + gamma * max Q(s’,a’)\n",
"        #Note that our target is a matrix, with possible fields not only for the action taken but also\n",
"        #for the other possible actions. The actions not take the same value as the prediction to not affect them\n",
"        targets = np.zeros((inputs.shape[0], num_actions))\n",
"\n",
"        #We draw states to learn from randomly\n",
"        for i, idx in enumerate(np.random.randint(0, len_memory,\n",
"                                                  size=inputs.shape[0])):\n",
"            \"\"\"\n",
"            Here we load one transition <s, a, r, s’> from memory\n",
"            state_t: initial state s\n",
"            action_t: action taken a\n",
"            reward_t: reward earned r\n",
"            state_tp1: the state that followed s’\n",
"            \"\"\"\n",
"            state_t, action_t, reward_t, state_tp1 = self.memory[idx][0]\n",
"\n",
"            #We also need to know whether the game ended at this state\n",
"            game_over = self.memory[idx][1]\n",
"\n",
"            #add the state s to the input\n",
"            inputs[i:i+1] = state_t\n",
"\n",
"            # First we fill the target values with the predictions of the model.\n",
"            # They will not be affected by training (since the training loss for them is 0)\n",
"            targets[i] = model.predict(state_t)[0]\n",
"\n",
"            \"\"\"\n",
"            If the game ended, the expected reward Q(s,a) should be the final reward r.\n",
"            Otherwise the target value is r + gamma * max Q(s’,a’)\n",
"            \"\"\"\n",
"            #  Here Q_sa is max_a'Q(s', a')\n",
"            Q_sa = np.max(model.predict(state_tp1)[0])\n",
"\n",
"            #if the game ended, the reward is the final reward\n",
"            if game_over:  # if game_over is True\n",
"                targets[i, action_t] = reward_t\n",
"            else:\n",
"                # r + gamma * max Q(s’,a’)\n",
"                targets[i, action_t] = reward_t + self.discount * Q_sa\n",
"        return inputs, targets\n",
"\n",
"epsilon = .1  # exploration\n",
"\n",
"def train(model, epochs, env, max_memory, batch_size, verbose = 1, visualize = False):\n",
"    exp_replay = ExperienceReplay(max_memory=max_memory)\n",
"    # Train\n",
"    #Reseting the win counter\n",
"    win_cnt = 0\n",
"    # We want to keep track of the progress of the AI over time, so we save its win count history\n",
"    win_hist = []\n",
"    #Epochs is the number of games we play\n",
"    for e in range(epochs):\n",
"        loss = 0.\n",
"        #Resetting the game\n",
"        env.reset()\n",
"        game_over = False\n",
"        # get initial input\n",
"        input_t = env.observe()\n",
"\n",
"        while not game_over:\n",
"            #The learner is acting on the last observed game screen\n",
"            #input_t is a vector containing representing the game screen\n",
"            input_tm1 = input_t\n",
"\n",
"            if np.random.rand() <= epsilon:\n",
"                # Select random move\n",
"                action = np.random.randint(0, 3, size=1) # random action\n",
"            else:\n",
"                # Select best move\n",
"                # q contains the expected rewards for the actions\n",
"                q = model.predict(input_tm1)\n",
"                # we pick the action with the highest expected reward\n",
"                action = np.argmax(q[0])\n",
"\n",
"            # apply action, get rewards and new state\n",
"            input_t, reward, game_over = env.act(action)\n",
"            # If we managed to catch the fruit we add 1 to our win counter\n",
"            if reward == 1:\n",
"                win_cnt += 1\n",
"\n",
"            if visualize:\n",
"                display_screen(action, reward, input_t)\n",
"\n",
"            \"\"\"\n",
"            The experiences < s, a, r, s’ > we make during gameplay are our training data.\n",
"            Here we first save the last experience, and then load a batch of experiences to train our model\n",
"            \"\"\"\n",
"\n",
"            # store experience\n",
"            exp_replay.remember([input_tm1, action, reward, input_t], game_over)\n",
"\n",
"            # Load batch of experiences\n",
"            inputs, targets = exp_replay.get_batch(model, batch_size=batch_size)\n",
"\n",
"            # train model on experiences\n",
"            batch_loss = model.train_on_batch(inputs, targets)\n",
"\n",
"            #print(loss)\n",
"            loss += batch_loss\n",
"        if verbose > 0:\n",
"            print(\"Epoch {:03d}/{:03d} | Loss {:.4f} | Win count {}\".format(e,epochs, loss, win_cnt))\n",
"        win_hist.append(win_cnt)\n",
"    return model\n",
"\n",
"\n",
"def test(model, env, visualize = True):\n",
"    #This function lets a pretrained model play the game to evaluate how well it is doing\n",
"    global last_frame_time\n",
"    #plt.ion()\n",
"    #c is a simple counter variable keeping track of how much we train\n",
"    c = 0\n",
"    #Reset the last frame time (we are starting from 0)\n",
"    last_frame_time = 0\n",
"    #Reset score\n",
"    points = 0\n",
"    #For training we are playing the game 10 times\n",
"    for e in range(10):\n",
"        #Reset the game\n",
"        env.reset()\n",
"        #The game is not over\n",
"        game_over = False\n",
"        # get initial input\n",
"        input_t = env.observe()\n",
"        #display_screen(3,points,input_t)\n",
"        c += 1\n",
"        while not game_over:\n",
"            #The learner is acting on the last observed game screen\n",
"            #input_t is a vector containing representing the game screen\n",
"            input_tm1 = input_t\n",
"            #Feed the learner the current status and get the expected rewards for different actions from it\n",
"            q = model.predict(input_tm1)\n",
"            #Select the action with the highest expected reward\n",
"            action = np.argmax(q[0])\n",
"            # apply action, get rewards and new state\n",
"            input_t, reward, game_over = env.act(action)\n",
"            #Update our score\n",
"            points += reward\n",
"            if visualize:\n",
"                display_screen(action,points,input_t)\n",
"            c += 1\n"
    ]
    }],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
